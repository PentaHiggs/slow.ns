{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Going to work only with sample/ first\n",
    "\n",
    "path = \"data/dogscatsredux/\"\n",
    "#path = \"data/dogscats/sample/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division, print_function\n",
    "import os, json\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "np.set_printoptions(precision=4, linewidth=100)\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n",
      "WARNING (theano.sandbox.cuda): The cuda backend is deprecated and will be removed in the next release (v0.10).  Please switch to the gpuarray backend. You can get more information about how to switch at this URL:\n",
      " https://github.com/Theano/Theano/wiki/Converting-to-the-new-gpu-back-end%28gpuarray%29\n",
      "\n",
      "Using gpu device 0: GeForce GTX 960 (CNMeM is disabled, cuDNN 5103)\n"
     ]
    }
   ],
   "source": [
    "from numpy.random import random, permutation\n",
    "from scipy import misc,ndimage\n",
    "from scipy.ndimage.interpolation import zoom\n",
    "\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading class names from fast.ai website\n",
    "\n",
    "FILES_PATH = 'http://files.fast.ai/models'\n",
    "CLASS_FILE='imagenet_class_index.json'\n",
    "\n",
    "import keras.utils.data_utils\n",
    "\n",
    "fpath = keras.utils.data_utils.get_file(CLASS_FILE, FILES_PATH+CLASS_FILE, cache_subdir='models')\n",
    "with open(fpath) as f:\n",
    "    class_dict = json.load(f)\n",
    "classes = [class_dict[str(i)][1] for i in range(len(class_dict))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets define the blocks that make up VGG16\n",
    "\n",
    "def ConvBlock(layers, model, filters):\n",
    "    for i in range(layers):\n",
    "        model.add(keras.layers.convolutional.ZeroPadding2D((1,1)))\n",
    "        model.add(keras.layers.convolutional.Convolution2D(filters, 3, 3, activation='relu'))\n",
    "    model.add(keras.layers.convolutional.MaxPooling2D((2,2), strides=(2,2)))\n",
    "    \n",
    "def FCBlock(model):\n",
    "    model.add(keras.layers.core.Dense(4096, activation='relu'))\n",
    "    model.add(keras.layers.core.Dropout(0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vgg_preprocessing(x):\n",
    "    x = x - np.array([123.68, 116.779, 103.939]).reshape((3,1,1))\n",
    "    return x[:, ::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def VGG_16():\n",
    "    model = keras.models.Sequential()\n",
    "    model.add(keras.layers.core.Lambda(vgg_preprocessing, input_shape=(3,224,224)))\n",
    "    \n",
    "    ConvBlock(2, model, 64)\n",
    "    ConvBlock(2, model, 128)\n",
    "    ConvBlock(3, model, 256)\n",
    "    ConvBlock(3, model, 512)\n",
    "    ConvBlock(3, model, 512)\n",
    "    \n",
    "    model.add(keras.layers.core.Flatten())\n",
    "    FCBlock(model)\n",
    "    FCBlock(model)\n",
    "    model.add(keras.layers.core.Dense(1000, activation='softmax'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tamie/Envs/fast.ai/local/lib/python3.5/site-packages/keras/layers/core.py:622: UserWarning: `output_shape` argument not specified for layer lambda_1 and cannot be automatically inferred with the Theano backend. Defaulting to output shape `(None, 3, 224, 224)` (same as input shape). If the expected output shape is different, specify it via the `output_shape` argument.\n",
      "  .format(self.name, input_shape))\n"
     ]
    }
   ],
   "source": [
    "# Instantiate model and populate it with downloaded weights\n",
    "fpath = keras.utils.data_utils.get_file('vgg16.h5', FILES_PATH+'vgg16.h5', cache_subdir=\"models\")\n",
    "model = VGG_16()\n",
    "model.load_weights(fpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, we're going to create a new Model based on this one, but such that\n",
    "# it does not go through the final dense_3 layer\n",
    "\n",
    "features_model = keras.models.Model(input=model.input,\n",
    "                       output=model.get_layer(\"dense_2\").output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model loaded.  It's BATCH TIME\n",
    "import keras.preprocessing.image\n",
    "\n",
    "batch_size = 64\n",
    "# Random seed to ensure consistency beteween get_batches calls if desired\n",
    "seed = 42\n",
    "\n",
    "def get_batches(dirname, gen=keras.preprocessing.image.ImageDataGenerator(), \n",
    "                shuffle=True, batch_size=batch_size, class_mode='categorical',\n",
    "                seed=None):\n",
    "    return gen.flow_from_directory(path+dirname, target_size=(224,224), class_mode=class_mode,\n",
    "                                   shuffle=shuffle, batch_size=batch_size, seed=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 22500 images belonging to 2 classes.\n",
      "Found 2500 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "batches = get_batches('train', seed=seed, batch_size=batch_size)\n",
    "val_batches = get_batches('valid', seed=seed, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 22500 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "# Success!  We have removed the final layer.  Now we have access to the entire previous 4096 output.\n",
    "# Lets try training an SVM on the top layer.  First thing we need to do to train this SVM is to\n",
    "# take the entire data set ans pass it through vgg to turn it into 4096-length vectors.\n",
    "\n",
    "# We regenerate the batches just in case (ensuring generator is at beginning)\n",
    "batches = get_batches('train', seed=42, batch_size=batch_size)\n",
    "features_vec = features_model.predict_generator(batches,batches.nb_sample)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 22500 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "batches = get_batches('train', seed=seed, batch_size=batch_size)\n",
    "\n",
    "num_images = batches.nb_sample\n",
    "\n",
    "binary_class_vec = np.full((num_images), -1., dtype=np.float32)\n",
    "#Iterate first amongst the batches, then through each batch\n",
    "\n",
    "num_batches = num_images // batch_size\n",
    "last_batch_size = num_images % batch_size\n",
    "\n",
    "for i in range(num_batches):\n",
    "    batch = next(batches)\n",
    "    binary_class_vec[i*batch_size:(i+1)*batch_size] = batch[1][:,0]\n",
    "        \n",
    "batch = next(batches)\n",
    "for j in range(last_batch_size):\n",
    "    binary_class_vec[(num_batches)*batch_size + j] = batch[1][j,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(22500, 4096)\n",
      "(22500,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma='auto', kernel='rbf',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# It's time to sklearn!\n",
    "\n",
    "from sklearn import svm\n",
    "classifier = svm.SVC()\n",
    "assert features_vec.shape[0] == binary_class_vec.shape[0]\n",
    "print(features_vec.shape)\n",
    "print(binary_class_vec.shape)\n",
    "classifier.fit(features_vec, binary_class_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2500 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "#Alright!  Lets get predictions for the validation batch and see how we did\n",
    "val_batches = get_batches('valid', seed=seed, batch_size=batch_size)\n",
    "\n",
    "#pass them through vgg\n",
    "val_features_vec = features_model.predict_generator(val_batches,val_batches.nb_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4096,)\n",
      "(2500, 4096)\n"
     ]
    }
   ],
   "source": [
    "print(val_features_vec[1].shape)\n",
    "print(val_features_vec.shape)\n",
    "# What do we get?\n",
    "pred_labels = classifier.predict(val_features_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2500 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "#Now, we want to compare these predicted lables to the original labels.\n",
    "#First, lets construct np.array with the ground truth\n",
    "\n",
    "val_batches = get_batches('valid', seed=seed, batch_size=batch_size)\n",
    "\n",
    "num_images = val_batches.nb_sample\n",
    "\n",
    "actual_labels = np.empty((num_images),dtype=np.float32)\n",
    "# Iterate first amongst the batches, then through each batch\n",
    "\n",
    "size_batches = num_images\n",
    "num_batches = num_images // batch_size\n",
    "last_batch_size = num_images % batch_size\n",
    "\n",
    "for i in range(num_batches):\n",
    "    val_batch = next(val_batches)\n",
    "    actual_labels[i*batch_size:(i+1)*batch_size] = val_batch[1][:,0]\n",
    "\n",
    "batch = next(val_batches)\n",
    "actual_labels[num_batches*batch_size:num_batches*batch_size+last_batch_size] = batch[1][:last_batch_size,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9852\n",
      "0.0770442315208\n"
     ]
    }
   ],
   "source": [
    "import sklearn.metrics\n",
    "accuracy = sklearn.metrics.accuracy_score(actual_labels, pred_labels)\n",
    "print(accuracy)\n",
    "# First attempt to make it work with logloss\n",
    "log_loss_labels = [accuracy if label==1 else 1-accuracy for label in pred_labels]\n",
    "log_loss = sklearn.metrics.log_loss(actual_labels, log_loss_labels)\n",
    "print(log_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "dec_func_out = classifier.decision_function(val_features_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.25617624025\n",
      "-3.48036890829\n",
      "1.0\n",
      "0.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(2500,)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(np.max(dec_func_out))\n",
    "print(np.min(dec_func_out))\n",
    "max_val = 2*np.max(dec_func_out)\n",
    "min_val = 2*abs(np.min(dec_func_out))\n",
    "rescaled_dec_func_out = [val/max_val+.5 if val > 0 else val/min_val+.5 for val in dec_func_out]\n",
    "print(np.max(rescaled_dec_func_out))\n",
    "print(np.min(rescaled_dec_func_out))\n",
    "pred_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.997377777778\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'train_acuracy' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m-------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-79-dd9b89073628>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_accuracy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mtrain_log_loss_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtrain_acuracy\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mtrain_accuracy\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_predicted_vec\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mtrain_log_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbinary_class_vec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_log_loss_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_log_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-79-dd9b89073628>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_accuracy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mtrain_log_loss_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtrain_acuracy\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mtrain_accuracy\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_predicted_vec\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mtrain_log_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbinary_class_vec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_log_loss_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_log_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_acuracy' is not defined"
     ]
    }
   ],
   "source": [
    "# Training set accuracy?\n",
    "\n",
    "#binary_class_vec = ground truth\n",
    "train_predicted_vec = classifier.predict(features_vec)\n",
    "train_accuracy = sklearn.metrics.accuracy_score(binary_class_vec, train_predicted_vec)\n",
    "print(train_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0182045703224\n"
     ]
    }
   ],
   "source": [
    "train_log_loss_labels = [train_accuracy if label==1 else 1-train_accuracy for label in train_predicted_vec]\n",
    "train_log_loss = sklearn.metrics.log_loss(binary_class_vec, train_log_loss_labels)\n",
    "print(train_log_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0390483181141\n"
     ]
    }
   ],
   "source": [
    "logistic_dec_func_out = 1 / (1 + np.exp(dec_func_out*(-4)))\n",
    "logistic_dec_func_out.shape\n",
    "print(sklearn.metrics.log_loss(actual_labels, logistic_dec_func_out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 12500 images belonging to 1 classes.\n"
     ]
    }
   ],
   "source": [
    "# Lets get the test data\n",
    "# We don't shuffle the test batch.  Good lord please don't shuffle a test batch ever again, ok thanks\n",
    "test_batches = get_batches(\"test1\", shuffle=False, batch_size=batch_size, class_mode=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cat': 0, 'dog': 1}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_batches.class_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, lets do our predictions:\n",
    "test_features = features_model.predict_generator(test_batches,test_batches.nb_sample)\n",
    "pred_test_labels = classifier.predict(test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.8067,  2.3676,  1.0699,  1.6231,  1.8809,  2.8071, -2.2384, -1.1745, -2.0824,  2.4267])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score_test_labels = classifier.decision_function(test_features)\n",
    "score_test_labels[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.,  1.,  1.,  1.,  1.,  1.,  0.,  0.,  0.,  1.], dtype=float32)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_test_labels[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wohoo predictions!  Lets save them to a file\n",
    "with open(path+'my_submission_2.csv', 'w') as f:\n",
    "    f.write(\"id,label\\n\")\n",
    "    for i, ele in enumerate(pred_test_labels):\n",
    "        f.write(\"{},\".format(i+1))\n",
    "        if (ele==1.):\n",
    "            f.write(str(.9852))\n",
    "            f.write(\"\\n\")\n",
    "        else:\n",
    "            f.write(str(.0148))\n",
    "            f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import FileLink\n",
    "FileLink(path+\"my_submission.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets try the magic this time!\n",
    "final_predictions = 1 / (1 + np.exp(score_test_labels*(-1)))\n",
    "with open(path+'my_submission_logistic.csv', 'w') as f:\n",
    "    f.write(\"id,label\\n\")\n",
    "    for i, ele in enumerate(final_predictions):\n",
    "        f.write(\"{},\".format(i+1))\n",
    "        f.write(str(\"%.4f\" % ele))\n",
    "        f.write(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
